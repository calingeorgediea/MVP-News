{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: ['loc', 'respinge', 'următor', 'reprezentant', 'sistem', 'Newsro', 'ac', 'urma']\n",
      "\n",
      "Topic: 1\n",
      "Words: ['loc', 'reprezentant', 'urma', 'ac', 'Newsro', 'sistem', 'următor', 'respinge']\n",
      "\n",
      "Topic: 2\n",
      "Words: ['sistem', 'respinge', 'urma', 'ac', 'următor', 'reprezentant', 'Newsro', 'loc']\n",
      "\n",
      "Topic: 3\n",
      "Words: ['sistem', 'loc', 'ac', 'Newsro', 'următor', 'urma', 'reprezentant', 'respinge']\n",
      "\n",
      "Topic: 4\n",
      "Words: ['respinge', 'Newsro', 'ac', 'următor', 'reprezentant', 'urma', 'loc', 'sistem']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from ast import literal_eval\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tokenized_output.csv')\n",
    "\n",
    "# Convert the 'tokens' column from string to list of tokens\n",
    "df['tokens'] = df['tokens'].apply(literal_eval)\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "# Filter out tokens that appear in less than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
    "\n",
    "# Check if corpus is not empty\n",
    "if not corpus:\n",
    "    print(\"Corpus is empty. Please check your data and filtering criteria.\")\n",
    "else:\n",
    "    # Train the LDA model\n",
    "    lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Print the topics and their corresponding words without weights\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        words = topic.split('+')\n",
    "        topic_words = [word.split('*')[1].replace('\"', '').strip() for word in words]\n",
    "        print(f'Topic: {idx}')\n",
    "        print(f'Words: {topic_words}')\n",
    "        print()\n",
    "\n",
    "    # Save the LDA model\n",
    "    lda_model.save('lda_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1799, -0.0813,  0.8690,  ..., -0.2820,  0.0934,  0.7427],\n",
      "         [ 0.5068,  0.4884,  0.1603,  ...,  0.3275,  0.6655, -0.3906],\n",
      "         [ 0.6516,  0.4548, -0.2504,  ...,  0.1670,  0.4344,  0.1669],\n",
      "         ...,\n",
      "         [ 0.2018, -0.9343, -0.4237,  ...,  1.2236, -0.0991, -0.1604],\n",
      "         [-0.6006, -0.3131,  0.0780,  ...,  0.5276, -0.4769,  0.3721],\n",
      "         [ 0.0914, -0.1258,  0.0552,  ...,  0.1965, -0.1086,  0.2170]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-9.8939e-01, -3.9414e-01,  3.7290e-01,  9.7404e-01, -1.6963e-01,\n",
      "          5.5492e-01,  1.1029e-01, -2.8783e-01,  3.9374e-01,  1.7747e-01,\n",
      "          3.0243e-01,  6.1234e-02, -4.5448e-01, -1.2506e-01,  1.5203e-01,\n",
      "         -3.6685e-01, -6.6735e-01,  7.7938e-01, -9.8936e-01,  4.1359e-01,\n",
      "          3.3446e-01,  2.6447e-01, -2.0789e-01,  4.5235e-01, -4.2164e-01,\n",
      "         -7.6997e-01, -3.4082e-01, -9.6521e-01, -8.3696e-01, -2.7217e-01,\n",
      "         -5.6633e-01, -8.7710e-01, -4.2994e-01,  7.4986e-01,  5.0664e-01,\n",
      "          3.9520e-01,  1.8714e-01, -7.0706e-01, -4.1648e-01,  9.7407e-01,\n",
      "         -9.9399e-01, -4.0390e-01,  4.8363e-02,  2.0990e-01,  9.9845e-01,\n",
      "         -8.3092e-01, -4.8101e-01, -6.9517e-02,  1.0468e-01,  5.0402e-01,\n",
      "          4.6814e-01, -9.6273e-01,  3.4119e-01, -3.3704e-02,  2.6879e-01,\n",
      "          3.0397e-01,  6.7163e-02, -9.9583e-01,  1.0376e-01,  5.6421e-01,\n",
      "          3.6079e-01,  3.3962e-02,  2.0875e-01,  1.3508e-01, -2.9880e-01,\n",
      "         -2.2494e-01, -7.4503e-01,  9.9871e-01, -6.2404e-01, -9.1243e-01,\n",
      "         -5.9210e-01, -2.7543e-01,  1.0488e-01,  3.6710e-01,  4.6854e-01,\n",
      "         -1.5204e-01, -1.0097e-01, -5.5973e-01,  4.8344e-01, -4.6334e-01,\n",
      "          2.4306e-01, -5.9118e-01,  9.9812e-01,  5.9355e-01, -7.8421e-01,\n",
      "         -1.2345e-02, -4.0732e-01, -1.2968e-01, -3.2135e-01, -9.5357e-01,\n",
      "          8.4139e-01, -9.0395e-01,  5.8730e-02, -5.3560e-01, -8.3967e-01,\n",
      "         -9.4520e-01,  5.6416e-01, -2.0918e-01,  2.0207e-01, -2.7010e-01,\n",
      "          4.4267e-01, -9.8715e-01, -6.2018e-01, -9.9166e-01, -5.8572e-01,\n",
      "         -2.0400e-01,  4.1328e-01, -4.5052e-01, -7.5013e-01,  2.3227e-01,\n",
      "         -4.0564e-01, -3.2079e-01,  1.2090e-01, -5.3478e-01, -2.6385e-03,\n",
      "         -9.9015e-01,  9.8216e-01,  2.1379e-01, -4.0204e-01, -4.6553e-01,\n",
      "          3.9032e-02,  1.7496e-01, -5.0736e-01,  9.8661e-01, -1.7497e-01,\n",
      "         -4.1790e-01, -4.8266e-01,  5.8264e-01, -8.0923e-01, -1.8058e-01,\n",
      "          2.4476e-01,  4.2144e-01,  9.9692e-01,  3.7334e-01,  3.2775e-01,\n",
      "         -2.0038e-01, -3.7277e-01, -1.2197e-01, -2.0116e-01,  4.4185e-01,\n",
      "          9.5199e-01,  9.1551e-01,  3.3994e-01,  5.9283e-02,  3.4902e-01,\n",
      "          9.6910e-01, -9.9926e-01,  9.4415e-01,  5.8949e-01,  5.2995e-02,\n",
      "         -2.8029e-02, -2.5603e-01,  9.7341e-01,  2.3918e-01, -4.6664e-01,\n",
      "          1.3836e-01,  9.8454e-01,  3.0650e-01,  4.1057e-01, -5.8494e-01,\n",
      "          2.3332e-01,  6.9368e-01,  1.6705e-01,  4.8945e-01,  3.2811e-02,\n",
      "         -4.7191e-01,  4.9717e-01, -3.1367e-01,  1.5129e-01,  9.8055e-01,\n",
      "         -4.5707e-01, -6.6337e-01, -5.9751e-01, -1.9528e-02, -3.4656e-01,\n",
      "          9.8799e-01, -3.3582e-01,  7.0936e-02,  5.6948e-02, -5.5648e-01,\n",
      "         -8.6423e-01, -5.3335e-01,  9.6713e-01, -9.9010e-01,  2.1096e-01,\n",
      "         -5.3302e-01,  5.6863e-01, -4.4332e-01, -8.9681e-01, -8.3652e-01,\n",
      "         -4.2399e-01, -4.8267e-01, -4.3977e-01,  6.8148e-01,  4.2076e-01,\n",
      "          4.6542e-01,  2.1420e-01, -9.9024e-01, -5.2636e-01,  1.9498e-01,\n",
      "          4.0581e-01,  3.9894e-01, -6.2216e-01, -9.0798e-01,  9.5290e-02,\n",
      "         -2.8018e-01,  7.5500e-01,  1.4613e-01,  2.2268e-01, -8.8064e-01,\n",
      "          4.8293e-01, -7.7262e-01,  8.3763e-01,  2.2669e-01, -1.1742e-01,\n",
      "          9.4196e-01, -4.5034e-01,  7.1381e-01,  9.4452e-01,  3.7806e-01,\n",
      "         -9.4914e-01, -7.5377e-02, -9.9253e-01, -4.5676e-01,  2.8368e-02,\n",
      "          9.5062e-02,  9.6943e-01, -9.7030e-01, -3.1009e-01,  9.1890e-01,\n",
      "         -9.0678e-02, -9.1197e-01, -8.0065e-01,  3.1156e-01,  7.5229e-02,\n",
      "         -1.0652e-01,  9.9679e-01, -1.8281e-01, -4.2056e-01,  8.6985e-01,\n",
      "         -8.3582e-01, -2.5597e-01, -9.8227e-01, -3.9549e-01,  6.1157e-01,\n",
      "         -2.4014e-01, -5.3044e-01, -1.8022e-01, -8.7433e-01, -3.4880e-02,\n",
      "         -4.9973e-01,  1.7801e-01,  2.1849e-01,  2.2977e-01,  9.9821e-01,\n",
      "         -3.2356e-01,  6.4803e-01,  3.8522e-01,  6.2593e-01, -1.0428e-02,\n",
      "          9.9560e-01, -5.1180e-01, -1.7925e-01,  1.0969e-01,  7.7377e-01,\n",
      "         -7.5411e-01,  2.9585e-01, -9.4509e-01, -6.8323e-01,  5.4045e-01,\n",
      "         -5.7901e-01,  4.5530e-01,  3.3743e-01,  2.4813e-01, -5.9281e-01,\n",
      "         -9.8555e-01, -7.3434e-01,  8.8369e-01, -1.5838e-01,  7.1910e-01,\n",
      "         -5.2288e-01, -9.8443e-01, -6.0831e-01,  2.9020e-01,  1.9398e-01,\n",
      "         -1.0468e-01, -3.8483e-01,  4.9891e-01, -8.0609e-01, -6.1325e-01,\n",
      "         -7.0757e-01,  3.5888e-01,  1.7640e-02,  5.9542e-02, -6.7188e-01,\n",
      "         -2.0137e-01, -5.5451e-03, -4.8806e-01, -3.7221e-01, -2.5300e-01,\n",
      "          3.4540e-02, -1.5940e-01, -4.7977e-01, -4.3223e-01,  4.8088e-01,\n",
      "         -3.6971e-01,  9.8893e-01,  5.0537e-01, -1.3115e-01, -6.5380e-01,\n",
      "         -7.6114e-01,  6.0155e-02,  4.4809e-01, -5.3674e-01,  3.3176e-01,\n",
      "         -2.0148e-01,  5.0868e-02,  1.7984e-01, -1.8583e-02, -8.8044e-01,\n",
      "          2.9577e-01, -7.2664e-01, -1.3082e-01, -2.9518e-01,  4.8792e-01,\n",
      "         -3.2199e-01,  5.9328e-02,  7.9800e-02, -7.5712e-01, -5.9197e-01,\n",
      "         -3.1314e-01, -1.7780e-01, -6.1304e-01, -1.2879e-01,  6.2703e-01,\n",
      "          9.9439e-01,  9.8995e-01,  2.4573e-01,  4.4049e-01,  6.0847e-01,\n",
      "         -5.5765e-02, -2.6183e-01, -3.2302e-01, -4.5229e-01,  2.8380e-01,\n",
      "          2.0724e-01, -9.8453e-01, -9.7291e-01, -1.7754e-01, -9.8127e-01,\n",
      "         -9.9954e-01,  3.1991e-03,  4.0885e-02, -2.9330e-01, -9.9354e-01,\n",
      "         -9.8602e-01,  5.3922e-01,  1.0633e-01,  3.0563e-01,  7.3848e-02,\n",
      "          5.2931e-03,  1.0006e-01, -9.9658e-01, -6.4654e-01,  2.9414e-01,\n",
      "          5.8592e-01,  9.6674e-01, -1.3437e-01,  6.1272e-01,  9.7709e-01,\n",
      "          2.0724e-01, -3.2810e-02,  1.9776e-01,  4.4433e-01, -6.9910e-01,\n",
      "          1.4376e-01,  9.9371e-01, -9.6254e-01,  4.5003e-01,  1.9785e-01,\n",
      "         -9.8441e-01,  3.5793e-01, -5.3210e-01,  5.6364e-01,  7.9289e-02,\n",
      "          6.3727e-02, -1.2602e-01,  1.7872e-01,  2.2224e-01, -7.3789e-01,\n",
      "          3.7422e-02, -1.4889e-01, -3.8599e-01,  1.7125e-01, -9.2844e-01,\n",
      "          1.0166e-01, -3.7523e-01,  2.6832e-01, -4.6311e-01,  8.8737e-01,\n",
      "         -5.7583e-01, -5.2245e-01,  1.3253e-01,  7.5918e-01, -3.4273e-01,\n",
      "          5.4240e-01,  9.1296e-01,  1.0599e-01, -2.9338e-01, -8.1123e-01,\n",
      "          3.8084e-01, -4.1248e-01, -1.5891e-01, -2.8415e-03,  4.7448e-01,\n",
      "          9.7291e-01, -4.1179e-01,  2.5643e-01,  1.3227e-01, -2.7843e-01,\n",
      "          9.5779e-01, -4.1900e-01,  8.7707e-01,  9.9621e-01, -4.4125e-01,\n",
      "          7.3915e-01,  4.6715e-01, -6.2430e-01, -9.5123e-01, -9.7191e-01,\n",
      "         -3.7545e-01, -2.5359e-01, -9.7434e-01,  3.8216e-01,  7.3060e-01,\n",
      "          1.9031e-01, -2.1620e-01,  3.9971e-01, -8.7858e-01,  9.8777e-01,\n",
      "         -8.6803e-01,  8.2636e-02, -2.5088e-01,  9.8582e-01,  3.4727e-01,\n",
      "         -6.7025e-01,  2.4133e-01, -3.2124e-01, -6.9826e-01, -8.3598e-01,\n",
      "          1.9576e-01, -9.5281e-02,  6.6298e-01,  4.0819e-02, -8.4958e-01,\n",
      "          1.0661e-01, -7.2868e-01, -4.3644e-01, -9.2836e-01, -7.9795e-01,\n",
      "         -2.2337e-01,  9.9235e-01, -5.4909e-01,  4.9432e-01,  9.4931e-01,\n",
      "         -2.5353e-01, -4.3980e-01,  1.7795e-01,  7.2711e-01, -2.4417e-01,\n",
      "         -9.4889e-02,  2.5791e-01,  2.6602e-02,  7.6205e-03, -5.7082e-01,\n",
      "         -2.4580e-01, -5.9306e-01, -6.7736e-04, -1.5638e-01,  4.2841e-01,\n",
      "         -2.3360e-01,  5.8597e-01, -3.9052e-01, -5.8359e-01, -5.2788e-01,\n",
      "         -9.4493e-01,  2.4004e-01,  5.9952e-01, -6.8209e-01, -8.1088e-01,\n",
      "          2.8640e-01, -8.2913e-01,  5.0016e-01, -5.0884e-02,  1.5098e-02,\n",
      "         -9.8002e-01, -3.3582e-01, -1.0252e-01,  9.7359e-01, -5.4935e-01,\n",
      "          1.2742e-01, -8.5269e-01,  5.2130e-01,  6.6240e-01,  1.9551e-01,\n",
      "          9.8072e-01,  4.6885e-01,  6.9482e-01,  6.2622e-01, -2.7960e-01,\n",
      "          1.3890e-01, -7.5244e-01,  3.3131e-01, -5.2587e-01, -3.1343e-01,\n",
      "         -9.6261e-01, -1.6342e-01,  1.4182e-02,  9.9564e-01, -9.0814e-01,\n",
      "         -1.0114e-01,  2.8499e-01, -3.8942e-01, -2.5212e-01, -5.6314e-01,\n",
      "          9.8749e-01, -2.8376e-01,  9.3974e-01,  9.0493e-01, -7.9081e-01,\n",
      "          9.8961e-01,  9.5418e-01,  4.0731e-01, -6.9517e-01,  3.6231e-02,\n",
      "          9.9441e-01, -5.2629e-01,  1.6212e-01,  3.3255e-01, -9.6932e-01,\n",
      "          1.3597e-01, -1.7664e-02, -7.2020e-01, -9.4354e-02,  6.2595e-02,\n",
      "          8.2093e-01,  1.3768e-01, -2.0845e-01,  3.5322e-01, -4.0640e-01,\n",
      "         -9.9882e-01, -9.6067e-01,  3.5389e-01,  9.8300e-01,  6.3996e-05,\n",
      "          4.5288e-01,  4.8257e-01,  6.3077e-01, -5.0271e-01, -3.2236e-01,\n",
      "          2.8900e-01,  4.0568e-01,  6.4831e-01, -1.3659e-01,  2.5362e-01,\n",
      "         -1.9974e-01, -8.4308e-02,  2.8359e-01, -1.1694e-01, -9.6913e-01,\n",
      "         -7.7330e-01,  4.8557e-02, -2.0039e-01,  3.0922e-01,  1.4281e-01,\n",
      "          1.7182e-01, -4.0093e-02,  9.7395e-01, -2.4738e-02,  1.2405e-01,\n",
      "         -3.2108e-01,  2.2621e-02, -1.3718e-02, -8.3050e-01,  7.4443e-01,\n",
      "         -1.6285e-02,  4.5442e-01,  2.1512e-01, -7.7294e-01, -1.0001e-01,\n",
      "         -1.8692e-01, -8.4845e-01, -9.4981e-02, -4.8702e-01, -2.3906e-01,\n",
      "          2.6753e-02,  9.9954e-01, -1.5299e-02, -2.8813e-01, -5.1691e-01,\n",
      "         -5.4196e-01, -2.4384e-01, -3.5227e-01,  4.2455e-02,  8.9147e-01,\n",
      "          6.5683e-01,  4.5079e-01,  5.8464e-01,  8.6342e-01,  9.2822e-01,\n",
      "          7.0054e-01, -2.0067e-01, -9.9222e-01, -4.2310e-01,  1.9235e-01,\n",
      "         -1.8848e-02, -4.3868e-01,  5.8560e-01,  2.6510e-01, -2.0610e-01,\n",
      "         -6.5886e-01,  6.6920e-01, -9.3640e-02, -1.7804e-02, -9.1535e-01,\n",
      "          9.8915e-01,  2.9785e-01, -8.2416e-02,  9.1669e-01,  6.2664e-01,\n",
      "         -4.8295e-01,  9.1824e-01,  5.4667e-01, -8.8524e-02,  5.0899e-01,\n",
      "         -5.6158e-01,  1.7834e-01, -4.3834e-01,  2.8786e-01,  9.6767e-01,\n",
      "         -9.8239e-01,  1.2651e-01, -4.6883e-01, -9.3539e-01,  8.0808e-01,\n",
      "         -1.2002e-02, -6.1157e-01, -6.0157e-01,  7.8389e-01,  6.4286e-01,\n",
      "          9.7896e-01,  5.2638e-01, -9.8487e-01, -9.9507e-01,  4.6774e-02,\n",
      "          6.3003e-01,  9.9793e-01, -8.6029e-01, -3.8438e-01, -9.9694e-01,\n",
      "         -7.2111e-02, -1.8871e-01, -8.3944e-01,  8.3768e-01, -4.8988e-02,\n",
      "          3.2537e-01,  5.3537e-01, -5.7497e-01, -1.1211e-01, -2.0527e-01,\n",
      "         -1.8178e-01, -9.3029e-01, -1.5862e-01,  3.4120e-01,  2.8895e-01,\n",
      "          8.3556e-01, -1.2737e-01, -2.9181e-01,  7.8496e-01,  1.2045e-01,\n",
      "         -2.0248e-02,  1.3603e-01,  6.5239e-01,  4.8208e-01,  7.6256e-01,\n",
      "          5.5857e-01, -4.0500e-01,  2.8774e-01, -2.9734e-01,  3.6988e-01,\n",
      "          8.6841e-01,  1.1116e-01,  9.8971e-01,  3.6927e-01,  6.7885e-02,\n",
      "          1.8767e-01, -4.1845e-01, -9.8657e-01,  9.8396e-01, -9.5740e-01,\n",
      "         -7.8953e-01, -3.4399e-01, -2.4216e-01, -2.3803e-02, -4.0577e-01,\n",
      "          1.2254e-01, -4.5066e-01,  8.8768e-01,  7.4394e-01, -2.1744e-01,\n",
      "         -3.9171e-02,  7.7158e-01,  4.9019e-01,  4.6370e-01, -9.4701e-01,\n",
      "         -6.2984e-01,  7.9210e-03,  2.7338e-01,  5.6685e-02,  6.7093e-03,\n",
      "          1.8401e-01,  3.7948e-01,  9.8119e-01, -9.8256e-01, -1.5336e-01,\n",
      "          4.9720e-02, -6.2169e-01,  6.1655e-01, -8.9495e-01,  4.0240e-01,\n",
      "          4.2017e-01,  3.7445e-01,  7.4257e-01, -4.7975e-02, -9.8714e-01,\n",
      "         -7.5998e-01, -9.9216e-01,  1.3109e-01, -1.9239e-01, -6.0732e-01,\n",
      "         -2.5811e-01, -3.6969e-01,  9.8246e-01,  6.8572e-01,  5.1705e-01,\n",
      "         -8.9980e-01,  1.3293e-01, -4.7153e-02,  6.4016e-01, -4.1449e-01,\n",
      "         -9.5843e-01, -4.9745e-01, -2.7465e-01,  6.1159e-01, -7.1515e-01,\n",
      "         -1.8982e-02, -7.8681e-01, -7.3371e-01, -3.9251e-01,  9.1015e-01,\n",
      "          1.1626e-01,  9.3616e-01, -6.9571e-01, -5.4948e-01,  1.6829e-01,\n",
      "          3.6586e-01, -6.1333e-01,  5.5698e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "model = AutoModel.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "inputs = tokenizer(\"exemplu de propoziție\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the RoBERTa tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "model = AutoModel.from_pretrained(\"readerbench/RoBERT-base\")\n",
    "\n",
    "# Example text data\n",
    "text_data = [\n",
    "    \"exemplu de propoziție 1\",\n",
    "    \"exemplu de propoziție 2\",\n",
    "    \"exemplu de propoziție 3\"\n",
    "]\n",
    "\n",
    "# List to store embeddings\n",
    "embeddings = []\n",
    "\n",
    "# Iterate through each sentence, obtain embeddings, and store\n",
    "for text in text_data:\n",
    "    # Tokenize and encode the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Pass the input through the RoBERTa model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the last hidden state (last layer's output)\n",
    "    last_hidden_state = outputs.last_hidden_state\n",
    "    \n",
    "    # Take the mean of the embeddings for each token in the sentence\n",
    "    sentence_embedding = torch.mean(last_hidden_state, dim=1).squeeze().numpy()\n",
    "    \n",
    "    embeddings.append(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Combine embeddings to represent each document\n",
    "document_embeddings = np.stack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: ['loc', 'sistem', 'respinge', 'Newsro', 'reprezentant', 'următor', 'ac', 'urma']\n",
      "\n",
      "Topic: 1\n",
      "Words: ['ac', 'loc', 'reprezentant', 'urma', 'următor', 'Newsro', 'sistem', 'respinge']\n",
      "\n",
      "Topic: 2\n",
      "Words: ['sistem', 'respinge', 'reprezentant', 'următor', 'ac', 'urma', 'Newsro', 'loc']\n",
      "\n",
      "Topic: 3\n",
      "Words: ['loc', 'reprezentant', 'următor', 'sistem', 'respinge', 'Newsro', 'ac', 'urma']\n",
      "\n",
      "Topic: 4\n",
      "Words: ['respinge', 'Newsro', 'următor', 'urma', 'ac', 'sistem', 'reprezentant', 'loc']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora, models\n",
    "from ast import literal_eval\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('tokenized_output.csv')\n",
    "\n",
    "# Convert the 'tokens' column from string to list of tokens\n",
    "df['tokens'] = df['tokens'].apply(literal_eval)\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(df['tokens'])\n",
    "\n",
    "# Filter out tokens that appear in less than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(tokens) for tokens in df['tokens']]\n",
    "\n",
    "# Check if corpus is not empty\n",
    "if not corpus:\n",
    "    print(\"Corpus is empty. Please check your data and filtering criteria.\")\n",
    "else:\n",
    "    # Create the TF-IDF model\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    \n",
    "    # Transform the corpus to TF-IDF representation\n",
    "    tfidf_corpus = tfidf[corpus]\n",
    "    \n",
    "    # Train the LDA model using TF-IDF\n",
    "    lda_model = models.LdaModel(tfidf_corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "    # Print the topics and their corresponding words without weights\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        words = topic.split('+')\n",
    "        topic_words = [word.split('*')[1].replace('\"', '').strip() for word in words]\n",
    "        print(f'Topic: {idx}')\n",
    "        print(f'Words: {topic_words}')\n",
    "        print()\n",
    "\n",
    "    # Save the LDA model\n",
    "    lda_model.save('lda_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: ['staţie', 'loc', 'metrou', 'minister', 'medic', 'persoană', 'vedere', 'panou', 'monta', 'punctaj']\n",
      "\n",
      "Topic: 1\n",
      "Words: ['locomotivă', 'tren', 'abur', 'brașov', 'fum', 'regal', 'loc', 'an', 'veni', 'transmite']\n",
      "\n",
      "Topic: 2\n",
      "Words: ['rinkevics', 'preşedinte', 'edgars', 'funcţie', 'letonia', 'ministru', 'alegere', 'ţară', 'declara', 'extern']\n",
      "\n",
      "Topic: 3\n",
      "Words: ['spune', 'economic', 'operator', 'mână', 'sursă', 'coleg', 'pune', 'întâmpla', 'încuraja', 'horia']\n",
      "\n",
      "Topic: 4\n",
      "Words: ['leu', 'guvern', 'sindicat', 'majorare', 'ofertă', 'salariu', 'brut', 'an', 'grevă', 'profesor']\n",
      "\n",
      "<spacy.lang.ro.Romanian object at 0x76e0941c49d0>\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from spacy.lang.ro.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the spaCy model for Romanian\n",
    "nlp = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "# Define your text data (list of strings)\n",
    "texts = pd.read_csv('tokenized_output.csv')\n",
    "\n",
    "# Tokenize, remove stopwords, and get document vectors\n",
    "processed_texts = []\n",
    "for text in texts['tokens']:\n",
    "    doc = nlp(text)\n",
    "    processed_text = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha]\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "# Create corpus of document vectors\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print the topics and their corresponding words without weights\n",
    "for idx, topic in lda_model.print_topics(10):\n",
    "    words = topic.split('+')\n",
    "    topic_words = [word.split('*')[1].replace('\"', '').strip() for word in words]\n",
    "    print(f'Topic: {idx}')\n",
    "    print(f'Words: {topic_words}')\n",
    "    print()\n",
    "\n",
    "# Save the LDA model\n",
    "lda_model.save('lda_model_spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score: 0.6826622617113627\n",
      "Perplexity: -6.624298077033246\n",
      "Topic: 0\n",
      "Words: ['spune', 'operator', 'economic', 'sursă', 'pune', 'coleg', 'horia', 'constantinescu', 'încet', 'obișnuit']\n",
      "\n",
      "Topic: 1\n",
      "Words: ['staţie', 'metrou', 'proiect', 'locomotivă', 'panou', 'persoană', 'monta', 'tren', 'element', 'informare']\n",
      "\n",
      "Topic: 2\n",
      "Words: ['loc', 'persoană', 'tate', 'minister', 'medic', 'bucureşti', 'punctaj', 'sănătaţe', 'medicină', 'familie']\n",
      "\n",
      "Topic: 3\n",
      "Words: ['rinkevics', 'preşedinte', 'edgars', 'letonia', 'funcţie', 'ţară', 'alegere', 'declara', 'deveni', 'extern']\n",
      "\n",
      "Topic: 4\n",
      "Words: ['leu', 'guvern', 'sindicat', 'majorare', 'ofertă', 'brut', 'an', 'educație', 'didactic', 'salariu']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from spacy.lang.ro.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the spaCy model for Romanian\n",
    "nlp = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "# Define your text data (list of strings)\n",
    "texts = pd.read_csv('tokenized_output.csv')\n",
    "\n",
    "# Tokenize, remove stopwords, and get document vectors\n",
    "processed_texts = []\n",
    "for text in texts['tokens']:\n",
    "    doc = nlp(text)\n",
    "    processed_text = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha]\n",
    "    processed_texts.append(processed_text)\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "# Create corpus of document vectors\n",
    "corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n",
    "\n",
    "# Calculate Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n",
    "\n",
    "# Calculate Perplexity\n",
    "perplexity = lda_model.log_perplexity(corpus)\n",
    "print(f'Perplexity: {perplexity}')\n",
    "\n",
    "# Print the topics and their corresponding words without weights\n",
    "for idx, topic in lda_model.print_topics(10):\n",
    "    words = topic.split('+')\n",
    "    topic_words = [word.split('*')[1].replace('\"', '').strip() for word in words]\n",
    "    print(f'Topic: {idx}')\n",
    "    print(f'Words: {topic_words}')\n",
    "    print()\n",
    "\n",
    "# Save the LDA model\n",
    "lda_model.save('lda_model_spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.100090675), (1, 0.10005186), (2, 0.10005265), (3, 0.10008407), (4, 0.5997208)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.models import CoherenceModel\n",
    "from spacy.lang.ro.stop_words import STOP_WORDS\n",
    "\n",
    "# Load the spaCy model for Romanian\n",
    "nlp = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "# Load the LDA model\n",
    "lda_model = models.LdaModel.load('lda_model_spacy')\n",
    "\n",
    "# Define function for input-output pipeline\n",
    "def lda_pipeline(input_text):\n",
    "    # Tokenize, remove stopwords, and get document vectors\n",
    "    doc = nlp(input_text)\n",
    "    processed_text = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha]\n",
    "    \n",
    "    # Create corpus of document vectors\n",
    "    text_corpus = [dictionary.doc2bow(processed_text)]\n",
    "    \n",
    "    # Get the topic distribution for the input text\n",
    "    topic_distribution = lda_model[text_corpus][0]\n",
    "    \n",
    "    return topic_distribution\n",
    "\n",
    "# Test the pipeline function\n",
    "input_text = \"Your input text goes here...\"\n",
    "output = lda_pipeline(input_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: ['preşedinte', 'rinkevics', 'salariu', 'declara', 'edgars', 'lege', 'ministru', 'miercuri', 'fapt', 'trebui']\n",
      "\n",
      "Topic: 1\n",
      "Words: ['staţie', 'metrou', 'persoană', 'proiect', 'panou', 'monta', 'majorare', 'guvern', 'informare', 'an']\n",
      "\n",
      "Topic: 2\n",
      "Words: ['leu', 'guvern', 'sindicat', 'majorare', 'an', 'ofertă', 'brut', 'educație', 'didactic', 'grevă']\n",
      "\n",
      "Topic: 3\n",
      "Words: ['loc', 'minister', 'medic', 'locomotivă', 'sănătaţe', 'punctaj', 'medicină', 'familie', 'tren', 'universitate']\n",
      "\n",
      "Topic: 4\n",
      "Words: ['persoană', 'staţie', 'metrou', 'tate', 'panou', 'bucureşti', 'monta', 'element', 'proiect', 'informare']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from spacy.lang.ro.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "\n",
    "def text_pipeline(data_file, num_topics=5, passes=15):\n",
    "    # Load the spaCy model for Romanian\n",
    "    nlp = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "    # Load your text data from CSV\n",
    "    texts = pd.read_csv(data_file)\n",
    "\n",
    "    # Tokenize, remove stopwords, and get document vectors\n",
    "    def process_text(text):\n",
    "        doc = nlp(text)\n",
    "        processed_text = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha]\n",
    "        return processed_text\n",
    "\n",
    "    processed_texts = [process_text(text) for text in texts['tokens']]\n",
    "\n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "    # Create corpus of document vectors\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "    # Print the topics and their corresponding words without weights\n",
    "    for idx, topic in lda_model.print_topics(10):\n",
    "        words = topic.split('+')\n",
    "        topic_words = [word.split('*')[1].replace('\"', '').strip() for word in words]\n",
    "        print(f'Topic: {idx}')\n",
    "        print(f'Words: {topic_words}')\n",
    "        print()\n",
    "\n",
    "    # Save the LDA model\n",
    "    lda_model.save('lda_model_spacy')\n",
    "\n",
    "# Example Usage:\n",
    "text_pipeline('tokenized_output.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0\n",
      "Words: ['leu', 'sindicat', 'guvern', 'salariu', 'ofertă', 'an', 'brut', 'grevă', 'salarial', 'profesor']\n",
      "\n",
      "Topic: 1\n",
      "Words: ['leu', 'sindicat', 'guvern', 'an', 'loc', 'ofertă', 'spune', 'grevă', 'majorare', 'didactic']\n",
      "\n",
      "Topic: 2\n",
      "Words: ['staţie', 'metrou', 'educație', 'persoană', 'monta', 'panou', 'majorare', 'brut', 'proiect', 'informare']\n",
      "\n",
      "Topic: 3\n",
      "Words: ['preşedinte', 'rinkevics', 'salariu', 'declara', 'edgars', 'miercuri', 'fapt', 'lege', 'bode', 'trebui']\n",
      "\n",
      "Topic: 4\n",
      "Words: ['leu', 'guvern', 'staţie', 'ofertă', 'didactic', 'metrou', 'an', 'sindicat', 'panou', 'proiect']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from spacy.lang.ro.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "\n",
    "def text_pipeline(data_file, num_topics=5, passes=15):\n",
    "    # Load the spaCy model for Romanian\n",
    "    nlp = spacy.load(\"ro_core_news_lg\")\n",
    "\n",
    "    # Load your text data from CSV\n",
    "    texts = pd.read_csv(data_file)\n",
    "\n",
    "    # Tokenize, remove stopwords, and get document vectors\n",
    "    def process_text(text):\n",
    "        doc = nlp(text)\n",
    "        processed_text = [token.text.lower() for token in doc if token.text.lower() not in STOP_WORDS and token.is_alpha]\n",
    "        return processed_text\n",
    "\n",
    "    processed_texts = [process_text(text) for text in texts['tokens']]\n",
    "\n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = corpora.Dictionary(processed_texts)\n",
    "\n",
    "    # Create corpus of document vectors\n",
    "    corpus = [dictionary.doc2bow(text) for text in processed_texts]\n",
    "\n",
    "    # Train the LDA model\n",
    "    lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=passes)\n",
    "\n",
    "    # Print the topics and their corresponding words without weights\n",
    "    for idx, topic in lda_model.print_topics(10):\n",
    "        words = topic.split('+')\n",
    "        topic_words = [word.split('*')[1].replace('\"', '').strip() for word in words]\n",
    "        print(f'Topic: {idx}')\n",
    "        print(f'Words: {topic_words}')\n",
    "        print()\n",
    "\n",
    "    # Save the LDA model\n",
    "    lda_model.save('lda_model_spacy')\n",
    "\n",
    "# Example Usage:\n",
    "text_pipeline('cool.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
