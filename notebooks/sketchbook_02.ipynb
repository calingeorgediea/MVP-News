{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0,\n",
      " 'eps': 8,\n",
      " 'graph': 10,\n",
      " 'human': 1,\n",
      " 'interface': 2,\n",
      " 'minors': 11,\n",
      " 'response': 3,\n",
      " 'survey': 4,\n",
      " 'system': 5,\n",
      " 'time': 6,\n",
      " 'trees': 9,\n",
      " 'user': 7}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "import pprint\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "# note that \"interaction\" does not appear since it's not found in the original dictionary.\n",
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import smart_open\n",
    "\n",
    "def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "    with smart_open.open(url, \"rb\") as file:\n",
    "        with tarfile.open(fileobj=file) as tar:\n",
    "            for member in tar.getmembers():\n",
    "                if member.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', member.name):\n",
    "                    member_bytes = tar.extractfile(member).read()\n",
    "                    yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1740\n",
      "387 \n",
      "Neural Net and Traditional Classifiers  \n",
      "William Y. Huang and Richard P. Lippmann \n",
      "MIT Lincoln Laboratory \n",
      "Lexington, MA 02173, USA \n",
      "Abstract\n",
      "Previous work on nets with continuous-valued inputs led to generative \n",
      "procedures to construct convex decision regions with two-layer percepttons (one hidden \n",
      "layer) and arbitrary decision regions with three-layer percepttons (two hidden layers). \n",
      "Here we demonstrate that two-layer perceptton classifiers trained with back propagation \n",
      "can form both c\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(docs[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 8644\n",
      "Number of documents: 1740\n"
     ]
    }
   ],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make an index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.1092.\n",
      "[([(0.009056574, 'gaussian'),\n",
      "   (0.007318024, 'mixture'),\n",
      "   (0.006725519, 'density'),\n",
      "   (0.006318043, 'likelihood'),\n",
      "   (0.005801836, 'prior'),\n",
      "   (0.005217883, 'bayesian'),\n",
      "   (0.0051224073, 'matrix'),\n",
      "   (0.004886573, 'estimate'),\n",
      "   (0.004717484, 'log'),\n",
      "   (0.004664063, 'component'),\n",
      "   (0.004513822, 'em'),\n",
      "   (0.004492974, 'class'),\n",
      "   (0.004217351, 'posterior'),\n",
      "   (0.004130018, 'approximation'),\n",
      "   (0.003884574, 'sample'),\n",
      "   (0.0037637085, 'variance'),\n",
      "   (0.0036264947, 'noise'),\n",
      "   (0.003384305, 'estimation'),\n",
      "   (0.0032502722, 'maximum'),\n",
      "   (0.0031151897, 'covariance')],\n",
      "  -0.8207048285312659),\n",
      " ([(0.0121412305, 'hidden'),\n",
      "   (0.007546479, 'layer'),\n",
      "   (0.006528153, 'speech'),\n",
      "   (0.0064861877, 'recognition'),\n",
      "   (0.006247099, 'hidden_unit'),\n",
      "   (0.005939379, 'net'),\n",
      "   (0.0051850905, 'rule'),\n",
      "   (0.0051401095, 'trained'),\n",
      "   (0.0049133054, 'word'),\n",
      "   (0.004616797, 'architecture'),\n",
      "   (0.003912561, 'sequence'),\n",
      "   (0.0037932412, 'classifier'),\n",
      "   (0.003663346, 'class'),\n",
      "   (0.0036108422, 'node'),\n",
      "   (0.0035659352, 'classification'),\n",
      "   (0.003472711, 'back'),\n",
      "   (0.003368613, 'gradient'),\n",
      "   (0.0033581806, 'training_set'),\n",
      "   (0.0033311276, 'propagation'),\n",
      "   (0.003304691, 'prediction')],\n",
      "  -0.9002364710575582),\n",
      " ([(0.02074169, 'neuron'),\n",
      "   (0.020148495, 'cell'),\n",
      "   (0.008503282, 'response'),\n",
      "   (0.008491939, 'spike'),\n",
      "   (0.0076550855, 'stimulus'),\n",
      "   (0.007182883, 'activity'),\n",
      "   (0.00693529, 'synaptic'),\n",
      "   (0.00641621, 'firing'),\n",
      "   (0.005397423, 'frequency'),\n",
      "   (0.005013427, 'cortex'),\n",
      "   (0.0049834414, 'connection'),\n",
      "   (0.0049100383, 'signal'),\n",
      "   (0.004358107, 'cortical'),\n",
      "   (0.0041304897, 'potential'),\n",
      "   (0.003724322, 'fig'),\n",
      "   (0.003604454, 'noise'),\n",
      "   (0.0034085494, 'visual'),\n",
      "   (0.0033568507, 'simulation'),\n",
      "   (0.0033430667, 'synapsis'),\n",
      "   (0.0033204928, 'excitatory')],\n",
      "  -0.9146430333339544),\n",
      " ([(0.02909814, 'image'),\n",
      "   (0.014362534, 'object'),\n",
      "   (0.009792523, 'visual'),\n",
      "   (0.007702768, 'map'),\n",
      "   (0.006832701, 'recognition'),\n",
      "   (0.0065462347, 'layer'),\n",
      "   (0.006180127, 'face'),\n",
      "   (0.0052770427, 'pixel'),\n",
      "   (0.004744349, 'view'),\n",
      "   (0.004574741, 'position'),\n",
      "   (0.004371549, 'orientation'),\n",
      "   (0.004251846, 'eye'),\n",
      "   (0.004063096, 'region'),\n",
      "   (0.0035630597, 'vision'),\n",
      "   (0.0035432398, 'scene'),\n",
      "   (0.0035010285, 'scale'),\n",
      "   (0.00345075, 'spatial'),\n",
      "   (0.0033409433, 'location'),\n",
      "   (0.0029583499, 'human'),\n",
      "   (0.0029191838, 'field')],\n",
      "  -1.0415724370875892),\n",
      " ([(0.0068661515, 'let'),\n",
      "   (0.0059960866, 'bound'),\n",
      "   (0.005868493, 'class'),\n",
      "   (0.005636381, 'optimal'),\n",
      "   (0.005592822, 'generalization'),\n",
      "   (0.005371101, 'theorem'),\n",
      "   (0.004963138, 'xi'),\n",
      "   (0.0047257836, 'approximation'),\n",
      "   (0.0044786483, 'sample'),\n",
      "   (0.004254823, 'convergence'),\n",
      "   (0.0040260833, 'loss'),\n",
      "   (0.0036187025, 'classifier'),\n",
      "   (0.003383809, 'estimate'),\n",
      "   (0.0032585608, 'solution'),\n",
      "   (0.0031478163, 'dimension'),\n",
      "   (0.0029974459, 'condition'),\n",
      "   (0.002932485, 'regression'),\n",
      "   (0.002795473, 'machine'),\n",
      "   (0.0027552664, 'noise'),\n",
      "   (0.0027211902, 'generalization_error')],\n",
      "  -1.1065356455543873),\n",
      " ([(0.0162753, 'circuit'),\n",
      "   (0.014279117, 'chip'),\n",
      "   (0.011639989, 'analog'),\n",
      "   (0.010879418, 'neuron'),\n",
      "   (0.008279525, 'voltage'),\n",
      "   (0.0066657173, 'vlsi'),\n",
      "   (0.0057795425, 'implementation'),\n",
      "   (0.0051703868, 'signal'),\n",
      "   (0.0051383157, 'processor'),\n",
      "   (0.00505459, 'bit'),\n",
      "   (0.004217481, 'design'),\n",
      "   (0.004174096, 'pulse'),\n",
      "   (0.0040155672, 'connection'),\n",
      "   (0.003955105, 'transistor'),\n",
      "   (0.003945803, 'parallel'),\n",
      "   (0.00391106, 'digital'),\n",
      "   (0.0038646979, 'hardware'),\n",
      "   (0.0038479462, 'synapse'),\n",
      "   (0.003565778, 'device'),\n",
      "   (0.0033233727, 'operation')],\n",
      "  -1.1401684434448056),\n",
      " ([(0.010432555, 'action'),\n",
      "   (0.008111158, 'policy'),\n",
      "   (0.0069046696, 'tree'),\n",
      "   (0.006886063, 'reinforcement'),\n",
      "   (0.0057749483, 'node'),\n",
      "   (0.005146549, 'optimal'),\n",
      "   (0.0046157893, 'reinforcement_learning'),\n",
      "   (0.004540131, 'decision'),\n",
      "   (0.0044481256, 'control'),\n",
      "   (0.0037487766, 'reward'),\n",
      "   (0.0031660197, 'environment'),\n",
      "   (0.0031421534, 'query'),\n",
      "   (0.0031125818, 'bound'),\n",
      "   (0.0030932836, 'goal'),\n",
      "   (0.00304588, 'search'),\n",
      "   (0.0030135408, 'machine'),\n",
      "   (0.0028152913, 'dynamic'),\n",
      "   (0.0027084171, 'robot'),\n",
      "   (0.0026833115, 'learn'),\n",
      "   (0.0026232223, 'sutton')],\n",
      "  -1.2044725918899133),\n",
      " ([(0.013068706, 'control'),\n",
      "   (0.0068037873, 'motor'),\n",
      "   (0.0061220857, 'movement'),\n",
      "   (0.005655151, 'trajectory'),\n",
      "   (0.0055986433, 'subject'),\n",
      "   (0.0055399453, 'target'),\n",
      "   (0.0054935063, 'position'),\n",
      "   (0.004998318, 'dynamic'),\n",
      "   (0.0049315547, 'controller'),\n",
      "   (0.004877685, 'human'),\n",
      "   (0.0047040763, 'word'),\n",
      "   (0.0046477504, 'direction'),\n",
      "   (0.0036183668, 'arm'),\n",
      "   (0.0034809022, 'forward'),\n",
      "   (0.0034223392, 'behavior'),\n",
      "   (0.0033966866, 'field'),\n",
      "   (0.0033923974, 'head'),\n",
      "   (0.003257837, 'activation'),\n",
      "   (0.0032090005, 'response'),\n",
      "   (0.0031987901, 'activity')],\n",
      "  -1.2143712884835614),\n",
      " ([(0.008337695, 'memory'),\n",
      "   (0.0076650693, 'signal'),\n",
      "   (0.0074224044, 'matrix'),\n",
      "   (0.0066078766, 'net'),\n",
      "   (0.006028562, 'neuron'),\n",
      "   (0.0052055046, 'dynamic'),\n",
      "   (0.004948933, 'component'),\n",
      "   (0.00477108, 'noise'),\n",
      "   (0.004712982, 'threshold'),\n",
      "   (0.004243621, 'source'),\n",
      "   (0.004152862, 'layer'),\n",
      "   (0.003938234, 'solution'),\n",
      "   (0.0035270057, 'capacity'),\n",
      "   (0.003402569, 'code'),\n",
      "   (0.0033226672, 'hopfield'),\n",
      "   (0.0033157235, 'rule'),\n",
      "   (0.0032926647, 'attractor'),\n",
      "   (0.0029751838, 'bit'),\n",
      "   (0.0028092281, 'energy'),\n",
      "   (0.0027833106, 'node')],\n",
      "  -1.351850163011038),\n",
      " ([(0.018969938, 'motion'),\n",
      "   (0.014021244, 'field'),\n",
      "   (0.012113739, 'distance'),\n",
      "   (0.011300135, 'character'),\n",
      "   (0.009447456, 'direction'),\n",
      "   (0.0080422, 'velocity'),\n",
      "   (0.0078964215, 'image'),\n",
      "   (0.00710801, 'receptive'),\n",
      "   (0.006746702, 'filter'),\n",
      "   (0.0066410834, 'receptive_field'),\n",
      "   (0.0063305316, 'tangent'),\n",
      "   (0.0053308727, 'digit'),\n",
      "   (0.0045802393, 'response'),\n",
      "   (0.0038860238, 'transformation'),\n",
      "   (0.0038619426, 'pixel'),\n",
      "   (0.0037609094, 'region'),\n",
      "   (0.0037112779, 'flow'),\n",
      "   (0.0036934877, 'center'),\n",
      "   (0.0036373388, 'spatial'),\n",
      "   (0.0036291431, 'moving')],\n",
      "  -1.397205251819178)]\n"
     ]
    }
   ],
   "source": [
    "top_topics = model.top_topics(corpus)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 'incepe',, 'loc',, 'transmite',, 'respinge',, 'spune',\n",
      "Topic 1: 'urma',, 'respinge',, 'transmite',, 'spune',, 'anunta',\n",
      "Topic 2: 'loc',, 'comunicat',, 'sistem',, 'intru',, 'respinge',\n",
      "Topic 3: 'respinge',, 'loc',, 'transmite',, 'incepe',, 'spune',\n",
      "Topic 4: 'respinge',, 'anunta',, 'incepe',, 'crestere',, 'urma',\n",
      "Topic 5: 'spune',, 'transmite',, 'urma',, 'sistem',, 'loc',\n",
      "Topic 6: 'spune',, 'transmite',, 'crestere',, 'intru',, 'comunicat',\n",
      "Topic 7: 'incepe',, 'spune',, 'loc',, 'respinge',, 'transmite',\n",
      "Topic 8: 'urma',, 'transmite',, 'loc',, 'anunta',, 'incepe',\n",
      "Topic 9: 'incepe',, 'loc',, 'spune',, 'urma',, 'transmite',\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Load the CSV file with tokenized content\n",
    "df = pd.read_csv(\"out.csv\")\n",
    "\n",
    "# Create a list of tokenized documents\n",
    "tokenized_docs = [doc.split() for doc in df[\"processed_content\"].values]\n",
    "\n",
    "# Create a dictionary representation of the documents\n",
    "dictionary = corpora.Dictionary(tokenized_docs)\n",
    "\n",
    "# Filter out tokens that appear in less than 5 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    "\n",
    "# Set number of topics\n",
    "num_topics = 10\n",
    "\n",
    "# Build the LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics,\n",
    "                         workers=3,  # Adjust based on your system\n",
    "                         passes=100)  # Number of passes through the corpus\n",
    "\n",
    "# Print the top 5 terms for each topic\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    terms = topic.split(\"+\")\n",
    "    terms = [term.split(\"*\")[1].strip().replace('\"', '') for term in terms][:5]\n",
    "    print(\"Topic {}: {}\".format(idx, \", \".join(terms)))\n",
    "\n",
    "# Save the model if needed\n",
    "# lda_model.save(\"lda_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
